{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ds.wesad.datasets import SubjectsDataset, subjects_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7ff38ab19bd0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_train = SubjectsDataset(subjects_data, ds_type=\"train\", step=5)\n",
    "ds_all_test = SubjectsDataset(subjects_data, ds_type=\"test\", step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/dmo/Documents/human_func_state/human_func_state\"\n",
    "epoch_count = 300\n",
    "optim_lr = 0.0001\n",
    "betas = \"default\"  # (0.85, 0.995) # Adam only\n",
    "train_batch_size = 8\n",
    "test_batch_size = 1\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_all_train,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = DataLoader(\n",
    "    ds_all_test,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvX(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_planes, out_planes, kernel=3, stride=1, padding=None\n",
    "    ):\n",
    "        super(ConvX, self).__init__()\n",
    "        padding = kernel // 2 if padding is None else padding\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_planes,\n",
    "            out_planes,\n",
    "            kernel_size=kernel,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net6Conv2MaxPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net6Conv2MaxPool, self).__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            ConvX(1, 1, kernel=2, padding=0),\n",
    "            ConvX(1, 1, kernel=2, padding=0),\n",
    "            nn.MaxPool1d(2),\n",
    "            ConvX(1, 1, kernel=2, padding=0),\n",
    "            ConvX(1, 1, kernel=2, padding=0),\n",
    "            nn.MaxPool1d(2),\n",
    "            ConvX(1, 1, kernel=2, padding=0),\n",
    "            ConvX(1, 1, kernel=2, stride=2),\n",
    "            nn.Conv1d(1, 1, kernel_size=2),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(torch.flatten(self.seq(x), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net6Conv2MaxPool().to(device=device)\n",
    "best_model = Net6Conv2MaxPool().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "mod_name = (\n",
    "    f\"{net.__class__.__name__}\"\n",
    "    f\"_{optimizer.__class__.__name__}\"\n",
    "    f\"_lr_{optim_lr}\"\n",
    ")\n",
    "write_path = f\"{base_path}/models_dumps/wesad/{mod_name}\"\n",
    "writer = SummaryWriter(log_dir=f\"{write_path}/hist\")\n",
    "dump_name = f\"{write_path}/{mod_name}.pkl\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model):\n",
    "    with torch.no_grad():\n",
    "        sum_ = 0\n",
    "        for i, data in enumerate(dl_test):\n",
    "            inputs, labels = data\n",
    "            inputs = (\n",
    "                torch.unsqueeze(inputs, 1).to(torch.float32).to(device=device)\n",
    "            )\n",
    "\n",
    "            outputs = model(inputs).cpu()\n",
    "            eq = outputs.max(1).indices == labels\n",
    "            sum_ += eq.sum()\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    epoch_count=50,\n",
    "    print_step=50,\n",
    "    start_epoch=0,\n",
    "    min_loss=(np.inf, 0),\n",
    "    best_rate=(0.0, 0),\n",
    "    worst_rate=(1.0, 0),\n",
    "):\n",
    "    # min_loss = _min_loss\n",
    "    # best_rate = _best_rate\n",
    "    # worst_rate = _worst_rate\n",
    "    len_test = len(dl_test) * dl_test.batch_size\n",
    "    for epoch in range(\n",
    "        start_epoch, epoch_count\n",
    "    ):  # loop over the dataset multiple times\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dl_train):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = (\n",
    "                torch.unsqueeze(inputs, 1).to(torch.float32).to(device=device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.cpu(), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_step == print_step - 1:\n",
    "                mean_loss = running_loss / print_step\n",
    "                if mean_loss < min_loss[0]:\n",
    "                    min_loss = (mean_loss, (epoch, i))\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {mean_loss:.3f}\")\n",
    "                epoch_loss += running_loss\n",
    "                running_loss = 0.0\n",
    "        epoch_loss += running_loss\n",
    "        model.eval()\n",
    "        count_of_correct = val(model)\n",
    "        model.train()\n",
    "        rate = count_of_correct / len_test\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", rate, epoch)\n",
    "        if rate > best_rate[0]:\n",
    "            best_rate = (rate, epoch)\n",
    "            with open(dump_name, \"wb\") as f:\n",
    "                state_dict = net.state_dict()\n",
    "                best_model.load_state_dict(copy.deepcopy(net.state_dict()))\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"net_state_dict\": state_dict,\n",
    "                        \"current_epoch\": epoch,\n",
    "                        \"rate\": rate,\n",
    "                        \"optimizer\": optimizer.__class__.__name__,\n",
    "                        \"optimizer_params\": optimizer.param_groups,\n",
    "                        \"train_batch_size\": train_batch_size,\n",
    "                        \"test_batch_size\": test_batch_size,\n",
    "                        \"device\": device,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "        if rate < worst_rate[0]:\n",
    "            worst_rate = (rate, epoch)\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch + 1}] rate: {rate:.4f} - {count_of_correct:3d}/{len_test}; {best_rate = }, {worst_rate = }\"\n",
    "        )\n",
    "    print(\"Finished Training. Min_loss:\", min_loss)\n",
    "    return worst_rate, best_rate, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.651\n",
      "[1,   100] loss: 0.611\n",
      "[1,   150] loss: 0.604\n",
      "[1,   200] loss: 0.593\n",
      "[1,   250] loss: 0.593\n",
      "[1,   300] loss: 0.583\n",
      "[1,   350] loss: 0.575\n",
      "[1,   400] loss: 0.583\n",
      "[1,   450] loss: 0.572\n",
      "[1,   500] loss: 0.571\n",
      "[1,   550] loss: 0.564\n",
      "[1,   600] loss: 0.550\n",
      "[1,   650] loss: 0.563\n",
      "[1,   700] loss: 0.568\n",
      "[1] rate: 0.0685 -  99/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0685), 0)\n",
      "[2,    50] loss: 0.543\n",
      "[2,   100] loss: 0.553\n",
      "[2,   150] loss: 0.548\n",
      "[2,   200] loss: 0.543\n",
      "[2,   250] loss: 0.560\n",
      "[2,   300] loss: 0.533\n",
      "[2,   350] loss: 0.531\n",
      "[2,   400] loss: 0.539\n",
      "[2,   450] loss: 0.555\n",
      "[2,   500] loss: 0.539\n",
      "[2,   550] loss: 0.535\n",
      "[2,   600] loss: 0.541\n",
      "[2,   650] loss: 0.550\n",
      "[2,   700] loss: 0.542\n",
      "[2] rate: 0.0609 -  88/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0609), 1)\n",
      "[3,    50] loss: 0.507\n",
      "[3,   100] loss: 0.541\n",
      "[3,   150] loss: 0.531\n",
      "[3,   200] loss: 0.538\n",
      "[3,   250] loss: 0.517\n",
      "[3,   300] loss: 0.527\n",
      "[3,   350] loss: 0.530\n",
      "[3,   400] loss: 0.527\n",
      "[3,   450] loss: 0.531\n",
      "[3,   500] loss: 0.535\n",
      "[3,   550] loss: 0.540\n",
      "[3,   600] loss: 0.540\n",
      "[3,   650] loss: 0.543\n",
      "[3,   700] loss: 0.523\n",
      "[3] rate: 0.0595 -  86/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0595), 2)\n",
      "[4,    50] loss: 0.537\n",
      "[4,   100] loss: 0.539\n",
      "[4,   150] loss: 0.531\n",
      "[4,   200] loss: 0.533\n",
      "[4,   250] loss: 0.514\n",
      "[4,   300] loss: 0.530\n",
      "[4,   350] loss: 0.527\n",
      "[4,   400] loss: 0.531\n",
      "[4,   450] loss: 0.527\n",
      "[4,   500] loss: 0.542\n",
      "[4,   550] loss: 0.516\n",
      "[4,   600] loss: 0.521\n",
      "[4,   650] loss: 0.525\n",
      "[4,   700] loss: 0.504\n",
      "[4] rate: 0.0533 -  77/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0533), 3)\n",
      "[5,    50] loss: 0.519\n",
      "[5,   100] loss: 0.526\n",
      "[5,   150] loss: 0.531\n",
      "[5,   200] loss: 0.544\n",
      "[5,   250] loss: 0.545\n",
      "[5,   300] loss: 0.524\n",
      "[5,   350] loss: 0.542\n",
      "[5,   400] loss: 0.516\n",
      "[5,   450] loss: 0.519\n",
      "[5,   500] loss: 0.503\n",
      "[5,   550] loss: 0.529\n",
      "[5,   600] loss: 0.521\n",
      "[5,   650] loss: 0.508\n",
      "[5,   700] loss: 0.524\n",
      "[5] rate: 0.0498 -  72/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0498), 4)\n",
      "[6,    50] loss: 0.506\n",
      "[6,   100] loss: 0.513\n",
      "[6,   150] loss: 0.532\n",
      "[6,   200] loss: 0.539\n",
      "[6,   250] loss: 0.529\n",
      "[6,   300] loss: 0.502\n",
      "[6,   350] loss: 0.509\n",
      "[6,   400] loss: 0.501\n",
      "[6,   450] loss: 0.536\n",
      "[6,   500] loss: 0.541\n",
      "[6,   550] loss: 0.543\n",
      "[6,   600] loss: 0.523\n",
      "[6,   650] loss: 0.511\n",
      "[6,   700] loss: 0.495\n",
      "[6] rate: 0.0484 -  70/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.0484), 5)\n",
      "[7,    50] loss: 0.537\n",
      "[7,   100] loss: 0.507\n",
      "[7,   150] loss: 0.513\n",
      "[7,   200] loss: 0.553\n",
      "[7,   250] loss: 0.509\n",
      "[7,   300] loss: 0.510\n",
      "[7,   350] loss: 0.518\n",
      "[7,   400] loss: 0.506\n",
      "[7,   450] loss: 0.506\n",
      "[7,   500] loss: 0.525\n",
      "[7,   550] loss: 0.507\n",
      "[7,   600] loss: 0.504\n",
      "[7,   650] loss: 0.531\n",
      "[7,   700] loss: 0.507\n",
      "[7] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[8,    50] loss: 0.508\n",
      "[8,   100] loss: 0.535\n",
      "[8,   150] loss: 0.516\n",
      "[8,   200] loss: 0.517\n",
      "[8,   250] loss: 0.518\n",
      "[8,   300] loss: 0.477\n",
      "[8,   350] loss: 0.520\n",
      "[8,   400] loss: 0.536\n",
      "[8,   450] loss: 0.527\n",
      "[8,   500] loss: 0.509\n",
      "[8,   550] loss: 0.509\n",
      "[8,   600] loss: 0.503\n",
      "[8,   650] loss: 0.529\n",
      "[8,   700] loss: 0.527\n",
      "[8] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[9,    50] loss: 0.538\n",
      "[9,   100] loss: 0.517\n",
      "[9,   150] loss: 0.524\n",
      "[9,   200] loss: 0.525\n",
      "[9,   250] loss: 0.545\n",
      "[9,   300] loss: 0.517\n",
      "[9,   350] loss: 0.519\n",
      "[9,   400] loss: 0.504\n",
      "[9,   450] loss: 0.500\n",
      "[9,   500] loss: 0.490\n",
      "[9,   550] loss: 0.510\n",
      "[9,   600] loss: 0.527\n",
      "[9,   650] loss: 0.504\n",
      "[9,   700] loss: 0.506\n",
      "[9] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[10,    50] loss: 0.522\n",
      "[10,   100] loss: 0.513\n",
      "[10,   150] loss: 0.525\n",
      "[10,   200] loss: 0.479\n",
      "[10,   250] loss: 0.499\n",
      "[10,   300] loss: 0.526\n",
      "[10,   350] loss: 0.525\n",
      "[10,   400] loss: 0.496\n",
      "[10,   450] loss: 0.496\n",
      "[10,   500] loss: 0.530\n",
      "[10,   550] loss: 0.519\n",
      "[10,   600] loss: 0.513\n",
      "[10,   650] loss: 0.526\n",
      "[10,   700] loss: 0.542\n",
      "[10] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[11,    50] loss: 0.525\n",
      "[11,   100] loss: 0.521\n",
      "[11,   150] loss: 0.508\n",
      "[11,   200] loss: 0.518\n",
      "[11,   250] loss: 0.514\n",
      "[11,   300] loss: 0.503\n",
      "[11,   350] loss: 0.518\n",
      "[11,   400] loss: 0.505\n",
      "[11,   450] loss: 0.534\n",
      "[11,   500] loss: 0.512\n",
      "[11,   550] loss: 0.501\n",
      "[11,   600] loss: 0.506\n",
      "[11,   650] loss: 0.509\n",
      "[11,   700] loss: 0.544\n",
      "[11] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[12,    50] loss: 0.498\n",
      "[12,   100] loss: 0.516\n",
      "[12,   150] loss: 0.512\n",
      "[12,   200] loss: 0.528\n",
      "[12,   250] loss: 0.489\n",
      "[12,   300] loss: 0.509\n",
      "[12,   350] loss: 0.506\n",
      "[12,   400] loss: 0.526\n",
      "[12,   450] loss: 0.501\n",
      "[12,   500] loss: 0.506\n",
      "[12,   550] loss: 0.504\n",
      "[12,   600] loss: 0.522\n",
      "[12,   650] loss: 0.515\n",
      "[12,   700] loss: 0.519\n",
      "[12] rate: 0.0000 -   0/1445; best_rate = (tensor(0.0685), 0), worst_rate = (tensor(0.), 6)\n",
      "[13,    50] loss: 0.519\n",
      "[13,   100] loss: 0.488\n",
      "[13,   150] loss: 0.525\n",
      "[13,   200] loss: 0.503\n",
      "[13,   250] loss: 0.519\n",
      "[13,   300] loss: 0.500\n",
      "[13,   350] loss: 0.519\n",
      "[13,   400] loss: 0.506\n",
      "[13,   450] loss: 0.511\n",
      "[13,   500] loss: 0.521\n",
      "[13,   550] loss: 0.525\n",
      "[13,   600] loss: 0.540\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch_count\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [12], line 22\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, epoch_count, print_step, start_epoch, min_loss, best_rate, worst_rate)\u001B[0m\n\u001B[1;32m     19\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     20\u001B[0m inputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(inputs, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m---> 22\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39mcpu(), labels)\n",
      "File \u001B[0;32m~/Documents/human_func_state/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:267\u001B[0m, in \u001B[0;36mOptimizer.zero_grad\u001B[0;34m(self, set_to_none)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m foreach:\n\u001B[1;32m    266\u001B[0m     per_device_and_dtype_grads \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28;01mlambda\u001B[39;00m: defaultdict(\u001B[38;5;28mlist\u001B[39m))\n\u001B[0;32m--> 267\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecord_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_zero_grad_profile_name\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    269\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "File \u001B[0;32m~/Documents/human_func_state/venv/lib/python3.10/site-packages/torch/autograd/profiler.py:478\u001B[0m, in \u001B[0;36mrecord_function.__init__\u001B[0;34m(self, name, args)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mrecord_function\u001B[39;00m(ContextDecorator):\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;124;03m\"\"\"Context manager/function decorator that adds a label to a block of\u001B[39;00m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;124;03m    Python code (or function) when running autograd profiler. It is\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;124;03m    useful when tracing the code profile.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    476\u001B[0m \n\u001B[1;32m    477\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 478\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m, args: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    479\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    480\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m args\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(net, epoch_count=epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 30])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dl_train):\n",
    "    inputs, labels = data\n",
    "    inputs = torch.unsqueeze(inputs, 1).to(torch.float32).to(device=device)\n",
    "    print(inputs.shape)\n",
    "    r = net(inputs)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9931, 0.0069],\n        [0.5000, 0.5000],\n        [0.5000, 0.5000],\n        [0.9931, 0.0069],\n        [0.9931, 0.0069],\n        [0.9931, 0.0069],\n        [0.9931, 0.0069],\n        [0.9931, 0.0069]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels > 1).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
