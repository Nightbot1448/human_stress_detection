{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from conv_nets.wesad_tries.utils import (\n",
    "    set_seed,\n",
    "    val,\n",
    "    save_data,\n",
    "    train_epoch,\n",
    ")\n",
    "from conv_nets.wesad_tries.models import get_model\n",
    "from ds.wesad.datasets import subjects_data\n",
    "from ds.wesad.datasets_users import SubjectDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fe8457ed3d0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_was_skipped_while_training = True\n",
    "data_key = \"rr_intervals\"\n",
    "numeric_derivative = True\n",
    "agg_type = \"max\"\n",
    "\n",
    "epoch_count = 50\n",
    "signal_len = 30\n",
    "optim_lr = 1e-6\n",
    "each_user_rate_history = True\n",
    "ds_step_size = 5\n",
    "train_batch_size = 8\n",
    "test_batch_size = 1\n",
    "device = \"cuda\"\n",
    "Model = get_model(signal_len, agg_type)\n",
    "Optimizer = optim.ASGD\n",
    "net_with_optim_name = f\"{Model.__name__}_{Optimizer.__name__}_lr_{optim_lr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "base_path = Path(\"/home/dmo/Documents/human_func_state/human_func_state\")\n",
    "wesad = base_path.joinpath(\"models_dumps\", \"wesad\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_and_dl(subject_id):\n",
    "    ds_subj_train = SubjectDataset(\n",
    "        subjects_data.get(subject_id),\n",
    "        ds_type=\"train\",\n",
    "        window_size=signal_len,\n",
    "        step=ds_step_size,\n",
    "        key=data_key,\n",
    "        numeric_derivative=numeric_derivative,\n",
    "    )\n",
    "    ds_subj_test = SubjectDataset(\n",
    "        subjects_data.get(subject_id),\n",
    "        ds_type=\"test\",\n",
    "        window_size=signal_len,\n",
    "        step=ds_step_size,\n",
    "        key=data_key,\n",
    "        numeric_derivative=numeric_derivative,\n",
    "    )\n",
    "    return {\n",
    "        \"ds\": {\n",
    "            \"train\": ds_subj_train,\n",
    "            \"test\": ds_subj_test,\n",
    "        },\n",
    "        \"dl\": {\n",
    "            \"train\": DataLoader(\n",
    "                ds_subj_train,\n",
    "                batch_size=train_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=1,\n",
    "                pin_memory=False,\n",
    "                drop_last=True,\n",
    "            ),\n",
    "            \"test\": DataLoader(\n",
    "                ds_subj_test,\n",
    "                batch_size=test_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=1,\n",
    "                pin_memory=False,\n",
    "                drop_last=True,\n",
    "            ),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_and_common_base_paths(\n",
    "    skip_user_on_train=False,\n",
    ") -> tuple[Path, Path]:\n",
    "    global net_with_optim_name\n",
    "    return (\n",
    "        wesad.joinpath(\n",
    "            *[\n",
    "                \"subjects_related\",\n",
    "                *([\"derivative\"] if numeric_derivative else []),\n",
    "                *([\"skip_users\"] if skip_user_on_train else []),\n",
    "                net_with_optim_name,\n",
    "            ]\n",
    "        ),\n",
    "        wesad.joinpath(\n",
    "            *[\n",
    "                \"steps\",\n",
    "                *([\"derivative\"] if numeric_derivative else []),\n",
    "                *([\"skip_users\"] if skip_user_on_train else []),\n",
    "                net_with_optim_name,\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "out_base_path, common_base_net_home = get_out_and_common_base_paths(\n",
    "    subjects_was_skipped_while_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_05_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_06_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_15_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_14_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_08_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_10_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_07_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_03_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/logs'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_17_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_02_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_16_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_09_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_04_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_11_NetUpDownCoder3MP_30_ASGD_lr_1e-06'),\n PosixPath('/home/dmo/Documents/human_func_state/human_func_state/models_dumps/wesad/steps/derivative/skip_users/NetUpDownCoder3MP_30_ASGD_lr_1e-06/subj_13_NetUpDownCoder3MP_30_ASGD_lr_1e-06')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(common_base_net_home.iterdir())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_net_state_dict(subject_id: int | None = None):\n",
    "    \"\"\"\n",
    "    Get net state dict. If `subject_id` specified then returns network\n",
    "    state with skipped subject while training\n",
    "\n",
    "    :param subject_id: skipped subject while training\n",
    "    :return: Common trained\n",
    "    \"\"\"\n",
    "    global net_with_optim_name\n",
    "    net_home = common_base_net_home / net_with_optim_name\n",
    "    if subject_id is not None:\n",
    "        net_home = common_base_net_home.joinpath(\n",
    "            f\"subj_{str(subject_id).zfill(2)}_{net_with_optim_name}\"\n",
    "        )\n",
    "    net_state_file = next(net_home.glob(\"*_best.pkl\"))\n",
    "    with open(net_state_file, \"rb\") as f:\n",
    "        return pickle.load(f).get(\"net_state_dict\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "stored = get_net_state_dict(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "new_d = net.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): ConvX(\n    (conv): Conv1d(1, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (1): ConvX(\n    (conv): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): ConvX(\n    (conv): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (4): ConvX(\n    (conv): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): ConvX(\n    (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (7): ConvX(\n    (conv): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (9): ConvX(\n    (conv): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (10): ConvX(\n    (conv): Conv1d(4, 2, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (11): Conv1d(2, 2, kernel_size=(3,), stride=(1,))\n)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.seq"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['seq.0.conv.weight', 'seq.0.bn.weight', 'seq.0.bn.bias', 'seq.0.bn.running_mean', 'seq.0.bn.running_var', 'seq.0.bn.num_batches_tracked', 'seq.1.conv.weight', 'seq.1.bn.weight', 'seq.1.bn.bias', 'seq.1.bn.running_mean', 'seq.1.bn.running_var', 'seq.1.bn.num_batches_tracked', 'seq.3.conv.weight', 'seq.3.bn.weight', 'seq.3.bn.bias', 'seq.3.bn.running_mean', 'seq.3.bn.running_var', 'seq.3.bn.num_batches_tracked', 'seq.4.conv.weight', 'seq.4.bn.weight', 'seq.4.bn.bias', 'seq.4.bn.running_mean', 'seq.4.bn.running_var', 'seq.4.bn.num_batches_tracked', 'seq.6.conv.weight', 'seq.6.bn.weight', 'seq.6.bn.bias', 'seq.6.bn.running_mean', 'seq.6.bn.running_var', 'seq.6.bn.num_batches_tracked', 'seq.7.conv.weight', 'seq.7.bn.weight', 'seq.7.bn.bias', 'seq.7.bn.running_mean', 'seq.7.bn.running_var', 'seq.7.bn.num_batches_tracked', 'seq.9.conv.weight', 'seq.9.bn.weight', 'seq.9.bn.bias', 'seq.9.bn.running_mean', 'seq.9.bn.running_var', 'seq.9.bn.num_batches_tracked', 'seq.10.conv.weight', 'seq.10.bn.weight', 'seq.10.bn.bias', 'seq.10.bn.running_mean', 'seq.10.bn.running_var', 'seq.10.bn.num_batches_tracked', 'seq.11.weight', 'seq.11.bias'])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3]) torch.Size([4, 1, 3])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([8, 4, 3]) torch.Size([8, 4, 3])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([16, 8, 3]) torch.Size([16, 8, 3])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([32, 16, 3]) torch.Size([32, 16, 3])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([32]) torch.Size([32])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([16, 32, 3]) torch.Size([16, 32, 3])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([16]) torch.Size([16])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([8, 16, 3]) torch.Size([8, 16, 3])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([4, 8, 3]) torch.Size([4, 8, 3])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([2, 4, 3]) torch.Size([2, 4, 3])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([]) torch.Size([])\n",
      "torch.Size([2, 2, 3]) torch.Size([2, 2, 3])\n",
      "torch.Size([2]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for s, n in zip(stored.values(), new_d.values()):\n",
    "    print(s.shape, n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.home().joinpath(\n",
    "    \"Documents\",\n",
    "    \"human_func_state\",\n",
    "    \"human_func_state\",\n",
    "    \"models_dumps\",\n",
    "    \"wesad\",\n",
    "    \"steps\",\n",
    "    \"derivative\",\n",
    "    \"NetUpDownCoder3MP_30_ASGD_lr_1e-06\",\n",
    "    \"NetUpDownCoder3MP_30_ASGD_lr_1e-06\",\n",
    "    \"NetUpDownCoder3MP_30_ASGD_lr_1e-06_best.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "with open(p, \"rb\") as f:\n",
    "    d = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('seq.0.conv.weight',\n              tensor([[[-1.4543e-01,  2.9138e-02, -2.3321e-02],\n                       [-2.8214e-01, -3.0476e-01, -9.3200e-01]],\n              \n                      [[ 7.8600e-03,  6.6469e-02, -5.7823e-01],\n                       [-5.4734e-01,  1.0128e-01, -2.6428e-01]],\n              \n                      [[-4.5640e-01,  8.6116e-02,  2.7025e-01],\n                       [-4.3448e-01,  2.7172e-02,  2.3120e-01]],\n              \n                      [[-4.8941e-05, -9.7109e-02,  3.9938e-01],\n                       [-1.6571e-01,  2.5700e-01,  3.7950e-01]]], device='cuda:0')),\n             ('seq.0.bn.weight',\n              tensor([0.9719, 1.0134, 1.0203, 0.9118], device='cuda:0')),\n             ('seq.0.bn.bias',\n              tensor([ 0.2658, -0.1680,  0.2002, -0.0864], device='cuda:0')),\n             ('seq.0.bn.running_mean',\n              tensor([-103.5825, -362.1432,  -94.1253,  221.6618], device='cuda:0')),\n             ('seq.0.bn.running_var',\n              tensor([ 2946.4312, 13009.2539,  6423.8413,  6211.1636], device='cuda:0')),\n             ('seq.0.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.1.conv.weight',\n              tensor([[[-0.2814,  0.2797,  0.1651],\n                       [ 0.3169,  0.1312,  0.2007],\n                       [-0.2384, -0.1966, -0.0053],\n                       [-0.1204, -0.1396,  0.1034]],\n              \n                      [[-0.1536,  0.1013, -0.0698],\n                       [-0.2109,  0.0495,  0.1604],\n                       [-0.2033,  0.3045, -0.0174],\n                       [-0.1701, -0.0210,  0.0889]],\n              \n                      [[ 0.1542,  0.1618,  0.0518],\n                       [-0.2273,  0.2105,  0.2284],\n                       [ 0.3397,  0.2026, -0.2019],\n                       [ 0.0686, -0.3256,  0.0231]],\n              \n                      [[-0.1202, -0.3486, -0.1959],\n                       [-0.0428,  0.1561, -0.1531],\n                       [-0.2994, -0.1638, -0.0653],\n                       [ 0.0421, -0.1103, -0.1985]],\n              \n                      [[-0.1661,  0.3612, -0.2732],\n                       [-0.0850,  0.4165, -0.0039],\n                       [-0.0324, -0.1279,  0.3768],\n                       [ 0.0420, -0.1893, -0.0306]],\n              \n                      [[ 0.0821, -0.0153, -0.0384],\n                       [-0.2272, -0.3137,  0.1153],\n                       [-0.3204,  0.2071,  0.0763],\n                       [-0.0833,  0.1598,  0.1734]],\n              \n                      [[ 0.2506,  0.0400, -0.2907],\n                       [ 0.1346, -0.2380,  0.2746],\n                       [-0.1826,  0.1521,  0.0282],\n                       [-0.0755, -0.2278, -0.2611]],\n              \n                      [[-0.1883, -0.1525,  0.0236],\n                       [ 0.1561,  0.2271, -0.3125],\n                       [-0.0154,  0.2636, -0.2124],\n                       [-0.1162,  0.1952,  0.0902]]], device='cuda:0')),\n             ('seq.1.bn.weight',\n              tensor([1.0269, 0.9809, 0.9590, 0.9608, 1.0551, 0.9517, 1.0288, 0.9627],\n                     device='cuda:0')),\n             ('seq.1.bn.bias',\n              tensor([ 0.0108, -0.1029,  0.0374,  0.0204, -0.0554, -0.0463,  0.0035,  0.0166],\n                     device='cuda:0')),\n             ('seq.1.bn.running_mean',\n              tensor([ 0.0180, -0.0309,  0.3720, -0.6435,  0.0841, -0.0492, -0.1176, -0.0692],\n                     device='cuda:0')),\n             ('seq.1.bn.running_var',\n              tensor([0.1883, 0.1328, 0.2113, 0.1771, 0.1645, 0.1855, 0.1337, 0.1286],\n                     device='cuda:0')),\n             ('seq.1.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.3.conv.weight',\n              tensor([[[-0.1050, -0.1083, -0.1544],\n                       [-0.0017,  0.1886, -0.1748],\n                       [ 0.0449,  0.1694,  0.1328],\n                       [ 0.1416,  0.1142, -0.0364],\n                       [-0.0168,  0.2029, -0.1170],\n                       [ 0.0783,  0.0437, -0.2247],\n                       [-0.1565,  0.0241, -0.1710],\n                       [ 0.1226, -0.0749, -0.0450]],\n              \n                      [[ 0.1910, -0.0304,  0.1319],\n                       [-0.1506,  0.0667, -0.1534],\n                       [ 0.1449,  0.1240,  0.1865],\n                       [-0.1802, -0.0605,  0.0248],\n                       [ 0.0885,  0.0836, -0.0100],\n                       [ 0.1363, -0.0094,  0.0969],\n                       [-0.1876,  0.2029,  0.1831],\n                       [-0.1814,  0.2107,  0.0250]],\n              \n                      [[ 0.0277, -0.0976,  0.0558],\n                       [ 0.1617,  0.0369,  0.2221],\n                       [ 0.0764,  0.0704,  0.0337],\n                       [ 0.1700, -0.0699,  0.0209],\n                       [-0.1133,  0.2017,  0.1902],\n                       [ 0.0565, -0.0730, -0.1048],\n                       [-0.1142,  0.2047,  0.2513],\n                       [-0.1254, -0.0293, -0.1189]],\n              \n                      [[-0.2169,  0.1906,  0.1597],\n                       [ 0.0143,  0.0783,  0.0688],\n                       [-0.2171, -0.1162, -0.1523],\n                       [ 0.1399, -0.1087, -0.1673],\n                       [ 0.0696,  0.0883,  0.1191],\n                       [-0.1916, -0.1482,  0.0673],\n                       [-0.1893, -0.0872, -0.0277],\n                       [-0.1497,  0.0324, -0.0395]],\n              \n                      [[-0.1581, -0.0443,  0.0434],\n                       [ 0.0233,  0.1543, -0.1763],\n                       [ 0.1259,  0.1836, -0.0841],\n                       [ 0.0885, -0.0933,  0.1497],\n                       [-0.0985, -0.0560, -0.1895],\n                       [-0.1457, -0.0627,  0.1070],\n                       [-0.1345,  0.1603,  0.0701],\n                       [-0.1697,  0.0944,  0.0490]],\n              \n                      [[ 0.0751, -0.2021,  0.0808],\n                       [ 0.1508,  0.0553,  0.1867],\n                       [-0.1537, -0.0168,  0.1858],\n                       [-0.1149,  0.0637,  0.0270],\n                       [-0.0511,  0.1289,  0.1384],\n                       [ 0.0005,  0.1707, -0.0008],\n                       [ 0.0435,  0.1218,  0.1528],\n                       [-0.0512,  0.0589,  0.0653]],\n              \n                      [[ 0.0890, -0.0106,  0.0838],\n                       [ 0.2166,  0.1229,  0.0993],\n                       [ 0.1654,  0.1126, -0.0696],\n                       [ 0.1143,  0.1982,  0.1782],\n                       [-0.0050, -0.2288, -0.0262],\n                       [ 0.2243,  0.1263,  0.1997],\n                       [ 0.0995,  0.0551,  0.2200],\n                       [-0.0013,  0.0729,  0.1261]],\n              \n                      [[-0.1871, -0.0101, -0.0809],\n                       [-0.1390, -0.1089,  0.0586],\n                       [ 0.0527, -0.0912, -0.1493],\n                       [ 0.1118, -0.0244, -0.0427],\n                       [-0.1004,  0.0947, -0.1696],\n                       [ 0.0326,  0.0907,  0.1499],\n                       [ 0.0941, -0.2042,  0.1495],\n                       [-0.0676,  0.1350,  0.2258]],\n              \n                      [[ 0.0900, -0.0635,  0.0645],\n                       [-0.1194,  0.1568, -0.1036],\n                       [ 0.1162, -0.0302,  0.1610],\n                       [-0.2050, -0.1392,  0.1847],\n                       [-0.1444, -0.2072, -0.1214],\n                       [-0.1371,  0.1753, -0.0183],\n                       [ 0.1872,  0.1911, -0.0582],\n                       [ 0.0190,  0.1695,  0.0287]],\n              \n                      [[-0.0692,  0.2495, -0.0369],\n                       [ 0.0629,  0.0452, -0.0530],\n                       [ 0.2106, -0.0098,  0.1461],\n                       [ 0.2156, -0.1096,  0.1206],\n                       [-0.1238,  0.1169,  0.1400],\n                       [-0.0023, -0.2119,  0.0210],\n                       [ 0.0517,  0.0141,  0.2978],\n                       [-0.0263,  0.1510,  0.0349]],\n              \n                      [[ 0.2028, -0.1496, -0.2164],\n                       [-0.0486, -0.0728, -0.1873],\n                       [ 0.0793, -0.1362, -0.1997],\n                       [ 0.1482, -0.2096,  0.0564],\n                       [ 0.1065, -0.0587, -0.1233],\n                       [-0.0789, -0.0711, -0.0570],\n                       [ 0.2562,  0.1391,  0.0402],\n                       [-0.2063, -0.0618, -0.0452]],\n              \n                      [[-0.1744, -0.1405, -0.1112],\n                       [ 0.0056,  0.0629,  0.1739],\n                       [-0.1167, -0.2265,  0.0417],\n                       [ 0.0963, -0.1417,  0.0946],\n                       [-0.1344,  0.1252, -0.1929],\n                       [ 0.2806, -0.0083, -0.0127],\n                       [ 0.1796, -0.0239, -0.1669],\n                       [-0.0291, -0.1450,  0.1309]],\n              \n                      [[-0.1580,  0.2028,  0.2198],\n                       [-0.1278,  0.0224, -0.0627],\n                       [-0.1897,  0.0243,  0.0235],\n                       [ 0.2423,  0.2293, -0.0771],\n                       [-0.1073,  0.0096,  0.3029],\n                       [-0.0998, -0.1751,  0.1345],\n                       [ 0.0388, -0.0173, -0.1314],\n                       [ 0.1114, -0.0383, -0.1265]],\n              \n                      [[ 0.1906,  0.0117, -0.2130],\n                       [ 0.2300,  0.1488,  0.1668],\n                       [ 0.1908,  0.1033, -0.0537],\n                       [-0.2114, -0.1120, -0.0196],\n                       [-0.0452, -0.1885, -0.1927],\n                       [ 0.0287,  0.0528, -0.1429],\n                       [ 0.0228,  0.0055,  0.1134],\n                       [-0.0139,  0.0582, -0.1667]],\n              \n                      [[ 0.1196, -0.0394, -0.2269],\n                       [ 0.0433,  0.0222,  0.0162],\n                       [ 0.0315, -0.1705, -0.2298],\n                       [ 0.0448,  0.1368,  0.0825],\n                       [-0.0215,  0.0979,  0.0447],\n                       [-0.0807, -0.1676,  0.0851],\n                       [ 0.0157,  0.0557, -0.1316],\n                       [ 0.1694, -0.0782, -0.0307]],\n              \n                      [[ 0.0829, -0.0044, -0.0577],\n                       [ 0.0723,  0.0091, -0.0855],\n                       [-0.0205,  0.0489, -0.1428],\n                       [ 0.0184, -0.0072,  0.1039],\n                       [-0.2410, -0.2328,  0.0350],\n                       [-0.0903, -0.1636, -0.1139],\n                       [-0.1029,  0.1609,  0.2007],\n                       [-0.2597, -0.2539, -0.0106]]], device='cuda:0')),\n             ('seq.3.bn.weight',\n              tensor([0.9873, 0.9537, 0.9843, 0.9768, 1.0032, 1.0184, 1.0004, 0.9822, 0.9704,\n                      0.9806, 1.0573, 0.9775, 0.9989, 0.9500, 0.9461, 1.0357],\n                     device='cuda:0')),\n             ('seq.3.bn.bias',\n              tensor([ 0.0057, -0.0284, -0.0106, -0.0743,  0.0043, -0.0376,  0.0308, -0.0072,\n                      -0.0181, -0.0465,  0.0314,  0.0092, -0.1149,  0.0026, -0.0553, -0.0565],\n                     device='cuda:0')),\n             ('seq.3.bn.running_mean',\n              tensor([-0.0474,  0.4770,  0.3557, -0.3666, -0.0059,  0.4387,  0.9267, -0.0772,\n                       0.1674,  0.5301, -0.3341, -0.2573,  0.1102, -0.0255, -0.1203, -0.4332],\n                     device='cuda:0')),\n             ('seq.3.bn.running_var',\n              tensor([0.1807, 0.3430, 0.1786, 0.1872, 0.2130, 0.1224, 0.3106, 0.2048, 0.2227,\n                      0.1689, 0.2167, 0.2579, 0.1933, 0.2498, 0.1720, 0.2091],\n                     device='cuda:0')),\n             ('seq.3.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.4.conv.weight',\n              tensor([[[ 0.0078, -0.1090, -0.0614],\n                       [-0.0063, -0.1162, -0.0620],\n                       [ 0.0263,  0.1442,  0.0091],\n                       ...,\n                       [-0.0094, -0.1221,  0.0152],\n                       [-0.1208, -0.0254,  0.0906],\n                       [-0.1231, -0.0483,  0.0355]],\n              \n                      [[ 0.0502,  0.0392, -0.0849],\n                       [-0.1168,  0.0936,  0.0825],\n                       [-0.1056, -0.1444,  0.0349],\n                       ...,\n                       [ 0.1069, -0.0234, -0.1106],\n                       [-0.1714,  0.1043,  0.1263],\n                       [ 0.1078, -0.0215, -0.0722]],\n              \n                      [[-0.0827, -0.0452, -0.0269],\n                       [-0.1280,  0.0205, -0.0338],\n                       [-0.0321,  0.0778,  0.1350],\n                       ...,\n                       [-0.0304, -0.0857,  0.1331],\n                       [ 0.0957, -0.0624, -0.0368],\n                       [-0.0494,  0.1491, -0.0666]],\n              \n                      ...,\n              \n                      [[ 0.0715, -0.0921, -0.1328],\n                       [-0.0472, -0.1490,  0.0721],\n                       [-0.0504, -0.0202,  0.0067],\n                       ...,\n                       [-0.0343,  0.0701, -0.1171],\n                       [-0.1330, -0.0542, -0.0944],\n                       [-0.0163,  0.1350, -0.0537]],\n              \n                      [[ 0.0777,  0.0715, -0.0943],\n                       [ 0.0156,  0.1460, -0.0018],\n                       [-0.0135,  0.0369,  0.0556],\n                       ...,\n                       [ 0.0120, -0.0539, -0.0560],\n                       [-0.0277,  0.1229, -0.0813],\n                       [-0.0225,  0.1311,  0.0641]],\n              \n                      [[ 0.0446,  0.0396,  0.0413],\n                       [ 0.0349, -0.0008,  0.0175],\n                       [-0.0416, -0.0138,  0.0269],\n                       ...,\n                       [ 0.1647,  0.0268,  0.0323],\n                       [ 0.0220,  0.1484,  0.0171],\n                       [-0.0752,  0.0573, -0.0271]]], device='cuda:0')),\n             ('seq.4.bn.weight',\n              tensor([0.9758, 0.9754, 0.9866, 0.9673, 0.9838, 0.9864, 0.9925, 1.0196, 0.9822,\n                      0.9892, 0.9894, 0.9814, 0.9891, 0.9780, 0.9758, 1.0012, 0.9896, 0.9976,\n                      0.9845, 0.9832, 0.9802, 0.9535, 0.9815, 1.0669, 1.0062, 0.9958, 0.9945,\n                      0.9918, 0.9923, 0.9922, 0.9780, 0.9879], device='cuda:0')),\n             ('seq.4.bn.bias',\n              tensor([-0.0124, -0.0055,  0.0023, -0.0327,  0.0137, -0.0074,  0.0084, -0.0047,\n                      -0.0168, -0.0038, -0.0012, -0.0055, -0.0097, -0.0163, -0.0215, -0.0163,\n                       0.0054, -0.0113, -0.0018,  0.0099, -0.0191, -0.0126, -0.0078,  0.0411,\n                      -0.0039, -0.0046, -0.0062, -0.0039, -0.0096, -0.0122, -0.0155, -0.0120],\n                     device='cuda:0')),\n             ('seq.4.bn.running_mean',\n              tensor([ 0.1628, -0.4503,  0.0443,  0.4763, -0.4127, -0.3130,  0.0922,  0.5376,\n                       0.1132, -0.3674,  0.0171,  0.2063,  0.4524, -0.0189, -0.2256,  0.1742,\n                       0.0741,  0.1909,  0.0452, -0.1365, -0.0677,  0.0950,  0.0075, -0.1664,\n                       0.1566, -0.3330,  0.0293, -0.1058,  0.0564, -0.1936,  0.2765,  0.4296],\n                     device='cuda:0')),\n             ('seq.4.bn.running_var',\n              tensor([0.1243, 0.1282, 0.2326, 0.1722, 0.2234, 0.1678, 0.1350, 0.1997, 0.1149,\n                      0.1232, 0.1174, 0.1753, 0.2034, 0.2095, 0.1542, 0.0930, 0.0901, 0.1213,\n                      0.1222, 0.1013, 0.0771, 0.1383, 0.2283, 0.2352, 0.0990, 0.2445, 0.1391,\n                      0.1427, 0.1798, 0.1510, 0.1205, 0.1406], device='cuda:0')),\n             ('seq.4.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.6.conv.weight',\n              tensor([[[ 0.0095,  0.0770,  0.1059],\n                       [-0.0038,  0.0306, -0.0991],\n                       [-0.0413,  0.0555, -0.0718],\n                       ...,\n                       [ 0.0299, -0.0273, -0.1292],\n                       [-0.0870,  0.0437, -0.0486],\n                       [ 0.0524, -0.0388,  0.0638]],\n              \n                      [[-0.0366, -0.0471, -0.0732],\n                       [ 0.0036,  0.0158,  0.0742],\n                       [ 0.0670, -0.0818, -0.0911],\n                       ...,\n                       [ 0.0863,  0.0437,  0.0457],\n                       [-0.0387,  0.0049,  0.0102],\n                       [ 0.0016,  0.0015, -0.0138]],\n              \n                      [[-0.0787, -0.0169, -0.0638],\n                       [ 0.0110,  0.0352, -0.0584],\n                       [-0.0767, -0.0107, -0.0882],\n                       ...,\n                       [ 0.0670,  0.0136, -0.1120],\n                       [ 0.0906, -0.0744,  0.0182],\n                       [ 0.0185, -0.0841, -0.1029]],\n              \n                      ...,\n              \n                      [[ 0.0127, -0.0715,  0.0833],\n                       [ 0.0091,  0.0391,  0.0637],\n                       [-0.0107, -0.0544, -0.0487],\n                       ...,\n                       [-0.1221, -0.0223,  0.0916],\n                       [ 0.0206, -0.0954, -0.0020],\n                       [-0.1104,  0.0643, -0.0060]],\n              \n                      [[-0.0420,  0.0515,  0.0941],\n                       [ 0.0784,  0.0498,  0.0885],\n                       [ 0.0675, -0.0754,  0.0270],\n                       ...,\n                       [ 0.0294,  0.0953, -0.0037],\n                       [-0.0637,  0.0333, -0.0897],\n                       [ 0.0478,  0.0629, -0.0747]],\n              \n                      [[ 0.0036, -0.0269,  0.0516],\n                       [-0.0819, -0.1079,  0.0916],\n                       [-0.0727,  0.0357,  0.0689],\n                       ...,\n                       [ 0.0368,  0.0166,  0.0486],\n                       [-0.0540,  0.0967,  0.0860],\n                       [ 0.0597,  0.0981,  0.0428]]], device='cuda:0')),\n             ('seq.6.bn.weight',\n              tensor([0.9991, 0.9698, 0.9877, 0.9720, 0.9924, 1.0014, 0.9785, 0.9825, 0.9700,\n                      1.0282, 1.0135, 1.0034, 0.9560, 0.9762, 0.9894, 1.0080],\n                     device='cuda:0')),\n             ('seq.6.bn.bias',\n              tensor([ 0.0205, -0.0282,  0.0053, -0.0174,  0.0073,  0.0209,  0.0107, -0.0384,\n                      -0.0071, -0.0029,  0.0198, -0.0052, -0.0088,  0.0012, -0.0036,  0.0125],\n                     device='cuda:0')),\n             ('seq.6.bn.running_mean',\n              tensor([ 0.5513, -0.0437, -0.4804, -0.2778,  0.5114, -0.2155, -0.2551,  0.2476,\n                       0.5960, -0.1705, -0.4384,  0.8834,  0.4178, -0.0106,  0.1045, -0.3448],\n                     device='cuda:0')),\n             ('seq.6.bn.running_var',\n              tensor([0.3041, 0.6470, 0.3402, 0.3592, 0.9361, 0.4840, 0.2764, 0.4260, 0.2725,\n                      0.7040, 0.6893, 0.2969, 0.2255, 0.1513, 0.4825, 0.6124],\n                     device='cuda:0')),\n             ('seq.6.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.7.conv.weight',\n              tensor([[[-1.8998e-01,  7.3725e-02, -3.1182e-02],\n                       [ 5.8457e-02,  1.3923e-01, -4.6728e-02],\n                       [-8.2589e-02, -8.1101e-02,  1.1912e-01],\n                       [ 4.2033e-02, -9.5742e-02,  9.9116e-02],\n                       [-1.3440e-01,  3.8820e-02, -8.2497e-03],\n                       [ 1.2998e-01,  1.0918e-02,  4.2098e-02],\n                       [ 7.4624e-02, -7.4622e-02,  1.3260e-01],\n                       [ 6.3564e-02,  8.0442e-03,  4.4602e-02],\n                       [-1.2468e-01,  9.4974e-02,  1.2413e-01],\n                       [-1.6013e-01, -1.3575e-01, -6.2536e-02],\n                       [-1.0764e-01, -4.3194e-02, -1.0732e-01],\n                       [-8.0904e-02,  4.0979e-02,  5.5752e-02],\n                       [-8.3165e-02, -3.7495e-02,  8.3767e-02],\n                       [-5.3888e-03, -7.3139e-03, -1.1235e-01],\n                       [ 1.9382e-03,  8.3448e-02, -1.7862e-02],\n                       [-2.9312e-02, -1.4425e-01,  1.6045e-01]],\n              \n                      [[ 5.5513e-02,  3.9276e-02, -5.2749e-02],\n                       [ 1.2954e-01, -2.5598e-02, -1.0403e-01],\n                       [ 3.7796e-02,  2.5936e-02,  9.8878e-03],\n                       [-1.0862e-01, -2.2095e-02,  2.1790e-02],\n                       [ 8.5147e-02,  3.3907e-02,  1.2999e-01],\n                       [-1.1710e-01,  8.7580e-02, -1.2354e-01],\n                       [ 5.1096e-03, -4.6344e-03, -4.1358e-02],\n                       [-7.3460e-02, -4.7498e-02,  1.3310e-03],\n                       [ 9.0249e-02, -8.0683e-02, -5.4606e-02],\n                       [-3.9742e-02,  1.0298e-01,  6.0102e-02],\n                       [ 9.5125e-02,  1.5003e-02, -8.9303e-02],\n                       [ 7.5181e-02, -1.1501e-01,  6.7963e-02],\n                       [ 4.6154e-02, -1.0855e-01, -6.3897e-02],\n                       [ 6.5776e-02,  8.3922e-03, -1.0720e-01],\n                       [-4.4395e-04, -6.9889e-02, -4.4613e-02],\n                       [-8.8898e-02, -8.2677e-02, -1.6546e-01]],\n              \n                      [[ 3.0030e-02, -8.5162e-02, -2.1668e-02],\n                       [-7.7066e-02,  6.5816e-02,  8.4198e-02],\n                       [ 4.4446e-03, -2.2770e-02, -1.1358e-01],\n                       [-1.1173e-01,  9.0539e-02, -5.7791e-02],\n                       [-4.0914e-02,  1.4563e-01,  5.6823e-02],\n                       [-7.6593e-02,  1.5476e-01,  3.7816e-02],\n                       [-1.1516e-01,  1.5978e-01, -1.5165e-02],\n                       [ 9.3898e-02,  2.2373e-02, -9.3671e-02],\n                       [-3.0421e-02, -8.2908e-02,  1.8537e-02],\n                       [ 3.3401e-02,  1.8172e-01,  2.4631e-02],\n                       [-6.7554e-02, -1.8703e-02, -3.0623e-02],\n                       [-5.9916e-02, -2.1113e-02, -2.8344e-02],\n                       [-8.9743e-02, -5.1520e-02,  7.2253e-02],\n                       [-1.1325e-01,  2.1324e-02, -6.0231e-02],\n                       [ 9.7582e-02,  6.9790e-02, -2.9541e-02],\n                       [ 9.9477e-02,  5.2033e-02,  1.0692e-01]],\n              \n                      [[-8.8347e-02, -4.1995e-03, -1.0752e-01],\n                       [ 8.0807e-02,  1.8171e-02,  4.1681e-02],\n                       [ 8.0633e-02, -1.3102e-01,  8.6550e-02],\n                       [-7.4681e-03, -7.6720e-03,  4.1158e-02],\n                       [-3.7315e-02, -1.7574e-01,  9.7405e-02],\n                       [-7.2245e-02, -2.7969e-03,  2.1561e-02],\n                       [ 7.5509e-03, -5.4452e-02,  2.8197e-02],\n                       [ 2.7918e-02,  2.7257e-02,  6.8330e-02],\n                       [-6.1031e-02, -4.8988e-02,  1.7570e-02],\n                       [-4.7487e-02, -1.6242e-01,  1.0887e-01],\n                       [-9.1641e-02,  1.1859e-01,  5.0004e-02],\n                       [ 6.1875e-02, -1.3533e-01, -6.4178e-02],\n                       [-8.7411e-02,  3.9554e-02,  1.1489e-01],\n                       [-6.4257e-03, -8.5882e-02,  9.5730e-02],\n                       [-4.1484e-02, -7.7250e-02,  1.1671e-01],\n                       [-2.7532e-02, -1.5914e-01, -1.0272e-01]],\n              \n                      [[-1.4145e-01, -1.5036e-01, -2.1861e-01],\n                       [ 6.2975e-02,  1.4255e-01, -7.0872e-02],\n                       [-7.6611e-02,  9.9691e-02, -1.2370e-01],\n                       [-7.2666e-02,  4.5111e-02, -3.1714e-02],\n                       [-7.0207e-03, -8.1951e-02, -6.8601e-02],\n                       [ 9.8641e-02, -2.3888e-02,  9.5457e-02],\n                       [-2.4954e-02,  6.7765e-02, -5.5623e-03],\n                       [ 6.0639e-02,  8.1170e-02,  1.6849e-01],\n                       [ 8.7707e-02,  2.3511e-02, -7.1559e-02],\n                       [-3.0974e-02,  1.2030e-01, -1.8222e-01],\n                       [ 1.7863e-01, -2.2147e-02,  1.5763e-01],\n                       [-1.4316e-01, -1.2382e-01, -1.2958e-01],\n                       [ 1.1741e-01,  3.8268e-02,  4.0530e-03],\n                       [-6.3604e-02, -1.0029e-01, -7.4523e-03],\n                       [ 3.1439e-03,  1.1564e-02, -1.3763e-01],\n                       [ 4.9855e-02,  7.7635e-02,  8.8899e-02]],\n              \n                      [[ 3.9036e-03, -4.6403e-02,  2.9238e-03],\n                       [-9.6835e-02,  1.4289e-01,  7.2341e-02],\n                       [ 5.3302e-02, -8.2542e-02,  6.4359e-02],\n                       [-7.7915e-02,  8.4069e-02, -1.3295e-01],\n                       [ 1.0687e-01,  1.0533e-01,  4.4554e-02],\n                       [ 1.4163e-01, -6.7779e-02,  8.6817e-02],\n                       [ 9.1026e-02,  5.7953e-02, -1.5255e-01],\n                       [-1.2875e-01, -5.1103e-02,  1.1983e-02],\n                       [-6.4546e-02,  1.0251e-01,  1.0529e-01],\n                       [ 2.0167e-01,  8.7055e-02,  1.7062e-01],\n                       [-1.5061e-01, -1.5425e-01, -1.0805e-01],\n                       [-5.2948e-02, -7.0981e-02, -7.3876e-02],\n                       [-1.0123e-01, -2.3572e-02, -6.0808e-02],\n                       [-4.2870e-02,  5.6431e-02, -7.9726e-02],\n                       [ 3.7036e-02,  2.1614e-02,  6.0384e-03],\n                       [ 1.7808e-01, -1.5084e-01, -1.0245e-01]],\n              \n                      [[-7.5137e-02, -9.4927e-02, -9.5143e-03],\n                       [ 9.9031e-02, -1.2401e-01,  6.7551e-02],\n                       [-8.6237e-02,  3.3097e-02, -1.0586e-01],\n                       [ 4.5291e-02, -4.1316e-02, -1.8327e-02],\n                       [ 1.1119e-01,  5.8802e-02, -5.5906e-02],\n                       [-4.0752e-02,  2.1239e-02,  9.4439e-02],\n                       [-3.8079e-02, -1.0708e-01, -1.3443e-02],\n                       [-1.5807e-02, -5.7098e-02,  1.6124e-01],\n                       [-9.1353e-02,  2.7006e-02, -1.1524e-01],\n                       [-5.9351e-02, -1.9610e-02,  4.8761e-02],\n                       [ 1.7751e-02,  3.8452e-02,  4.3369e-02],\n                       [-1.3856e-01, -1.0753e-01, -1.2582e-02],\n                       [ 1.2974e-01, -5.7645e-03, -1.9698e-02],\n                       [-9.6396e-02, -3.9555e-02,  9.3923e-02],\n                       [-8.4484e-02, -1.3047e-01, -7.6674e-02],\n                       [-1.3147e-01, -9.2690e-02, -1.0301e-01]],\n              \n                      [[ 1.2420e-01,  1.3424e-01, -4.0887e-02],\n                       [-2.2321e-02,  6.4157e-02, -7.6800e-02],\n                       [ 3.2255e-02,  1.6718e-02, -6.9771e-02],\n                       [ 8.8927e-02, -7.5963e-02, -9.6834e-02],\n                       [-1.5472e-01, -3.7062e-02,  4.9150e-02],\n                       [ 9.6037e-05,  1.0034e-01, -6.0867e-04],\n                       [ 3.9909e-02, -3.6119e-02, -1.0446e-01],\n                       [-4.3212e-02, -1.1993e-01, -1.4705e-01],\n                       [-1.1421e-01, -2.1475e-02, -1.0668e-01],\n                       [ 8.0881e-02,  1.0623e-01,  5.4832e-02],\n                       [ 9.3041e-02, -2.0144e-02, -1.6415e-01],\n                       [ 4.7337e-02, -1.0582e-01, -8.8177e-02],\n                       [-2.4891e-03, -1.3038e-01,  4.2068e-02],\n                       [-2.7567e-02,  8.7388e-02,  1.6024e-01],\n                       [-1.6494e-01, -1.1171e-01, -4.0255e-02],\n                       [-2.6638e-02, -6.3545e-02, -6.9281e-04]]], device='cuda:0')),\n             ('seq.7.bn.weight',\n              tensor([0.9768, 0.9717, 0.9889, 0.9600, 1.0257, 1.0269, 0.9595, 1.0053],\n                     device='cuda:0')),\n             ('seq.7.bn.bias',\n              tensor([-0.0075,  0.0362, -0.0046, -0.0022, -0.0074, -0.0232, -0.0128, -0.0096],\n                     device='cuda:0')),\n             ('seq.7.bn.running_mean',\n              tensor([-0.1827, -0.1336,  0.0319, -0.2569, -0.1216, -0.0100, -0.3720, -0.2536],\n                     device='cuda:0')),\n             ('seq.7.bn.running_var',\n              tensor([0.1620, 0.1735, 0.3520, 0.2454, 0.2480, 0.4643, 0.1725, 0.2444],\n                     device='cuda:0')),\n             ('seq.7.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.9.conv.weight',\n              tensor([[[-0.1726, -0.0085, -0.0678],\n                       [-0.0618, -0.2351, -0.0811],\n                       [-0.0612, -0.0681, -0.0101],\n                       [-0.1638,  0.0294, -0.1092],\n                       [ 0.0086,  0.1307,  0.0457],\n                       [-0.1545, -0.1956, -0.1834],\n                       [-0.1656,  0.1476,  0.1017],\n                       [ 0.1414, -0.1306, -0.2289]],\n              \n                      [[-0.0456, -0.1849, -0.1824],\n                       [-0.1384,  0.0642,  0.0724],\n                       [-0.1978,  0.1968, -0.0744],\n                       [-0.0063, -0.1435,  0.0562],\n                       [ 0.1196, -0.0194, -0.2489],\n                       [ 0.2112,  0.2528,  0.2109],\n                       [ 0.0585, -0.0617, -0.0605],\n                       [ 0.2070, -0.1106,  0.0993]],\n              \n                      [[ 0.2297, -0.1024, -0.0042],\n                       [-0.0409,  0.0641,  0.1831],\n                       [ 0.1621,  0.0912,  0.1094],\n                       [-0.1485, -0.0958, -0.1679],\n                       [ 0.0006, -0.0063, -0.1953],\n                       [ 0.1084,  0.0838,  0.0711],\n                       [-0.1914, -0.0515,  0.0441],\n                       [-0.0115, -0.1251,  0.0650]],\n              \n                      [[-0.0975,  0.0283, -0.0055],\n                       [ 0.1055, -0.0311, -0.0785],\n                       [-0.2218,  0.1082, -0.0088],\n                       [ 0.0718, -0.0337,  0.1734],\n                       [-0.1401,  0.1102,  0.2869],\n                       [-0.0960, -0.0679, -0.1021],\n                       [-0.1169, -0.0331,  0.1109],\n                       [ 0.1033,  0.0702, -0.2204]]], device='cuda:0')),\n             ('seq.9.bn.weight',\n              tensor([0.9763, 0.9388, 1.0078, 1.0621], device='cuda:0')),\n             ('seq.9.bn.bias',\n              tensor([-0.0052, -0.0854,  0.0581,  0.0023], device='cuda:0')),\n             ('seq.9.bn.running_mean',\n              tensor([-0.7314,  0.0424, -0.0155, -0.0192], device='cuda:0')),\n             ('seq.9.bn.running_var',\n              tensor([0.3558, 0.3815, 0.4982, 0.3417], device='cuda:0')),\n             ('seq.9.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.10.conv.weight',\n              tensor([[[ 0.0400, -0.1528,  0.1225],\n                       [-0.2136,  0.1967,  0.1625],\n                       [-0.1364,  0.0564,  0.3174],\n                       [ 0.2959,  0.0336,  0.4207]],\n              \n                      [[ 0.0358, -0.0604,  0.1390],\n                       [-0.1703,  0.0830,  0.1108],\n                       [-0.1064,  0.0561,  0.4107],\n                       [ 0.2598, -0.0121,  0.4068]]], device='cuda:0')),\n             ('seq.10.bn.weight', tensor([1.8291, 1.6250], device='cuda:0')),\n             ('seq.10.bn.bias', tensor([0.4739, 0.3850], device='cuda:0')),\n             ('seq.10.bn.running_mean',\n              tensor([0.2715, 0.2927], device='cuda:0')),\n             ('seq.10.bn.running_var',\n              tensor([0.2231, 0.1869], device='cuda:0')),\n             ('seq.10.bn.num_batches_tracked', tensor(10620, device='cuda:0')),\n             ('seq.11.weight',\n              tensor([[[ 1.6506, -0.3996, -0.0317],\n                       [ 1.3553, -0.2472,  0.0589]],\n              \n                      [[-0.1313,  0.1392,  0.3876],\n                       [-0.1540,  0.0273,  0.0290]]], device='cuda:0')),\n             ('seq.11.bias', tensor([ 0.2812, -0.7659], device='cuda:0'))])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get(\"net_state_dict\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name = (\n",
    "    f\"_{net.__class__.__name__}\"\n",
    "    f\"_{optimizer.__class__.__name__}\"\n",
    "    f\"_lr_{optim_lr}\"\n",
    ")\n",
    "subj_mod_name = f\"subj_{subject_id}_{mod_name}\"\n",
    "\n",
    "write_path = base_path.joinpath(\n",
    "    \"models_dumps\", \"wesad\", \"subjects_related\", mod_name, subj_mod_name\n",
    ")\n",
    "writer = SummaryWriter(log_dir=write_path / \"hist\")\n",
    "dump_name = (write_path / f\"{mod_name}_last\").with_suffix(\".pkl\")\n",
    "best_name = (write_path / f\"{mod_name}_best\").with_suffix(\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model: Model, dl):\n",
    "    with torch.no_grad():\n",
    "        sum_ = 0\n",
    "        for i, data in enumerate(dl):\n",
    "            inputs, labels = data\n",
    "            inputs = (\n",
    "                torch.unsqueeze(inputs, 1).to(torch.float32).to(device=device)\n",
    "            )\n",
    "\n",
    "            outputs = model(inputs).cpu()\n",
    "            eq = outputs.max(1).indices == labels\n",
    "            sum_ += eq.sum()\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.6618)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = val(net, dl_test)\n",
    "v / len(ds_subj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "45"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, net_state_dict, epoch, rate, rate_subject=None):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"net_state_dict\": net_state_dict,\n",
    "                \"current_epoch\": epoch,\n",
    "                \"rate\": rate,\n",
    "                \"rate_subject\": rate_subject,\n",
    "                \"optimizer\": optimizer.__class__.__name__,\n",
    "                \"optimizer_params\": optimizer.param_groups,\n",
    "                \"train_batch_size\": train_batch_size,\n",
    "                \"test_batch_size\": test_batch_size,\n",
    "                \"device\": device,\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    epoch_count=50,\n",
    "    print_step=50,\n",
    "    start_epoch=0,\n",
    "    min_loss=(torch.tensor(torch.inf), 0),\n",
    "    best_rate=(torch.tensor(0.0), 0),\n",
    "    worst_rate=(torch.tensor(1.0), 0),\n",
    "):\n",
    "    for epoch in trange(\n",
    "        start_epoch, epoch_count\n",
    "    ):  # loop over the dataset multiple times\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, data in enumerate(dl_train):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = (\n",
    "                torch.unsqueeze(inputs, 1).to(torch.float32).to(device=device)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.cpu(), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_step == print_step - 1:\n",
    "                mean_loss = running_loss / print_step\n",
    "                if mean_loss < min_loss[0]:\n",
    "                    min_loss = (mean_loss, (epoch, i))\n",
    "                print(f\"[{epoch:3d}, {i:4d}] loss: {mean_loss:.3f}\")\n",
    "                epoch_loss += running_loss\n",
    "                running_loss = 0.0\n",
    "        epoch_loss += running_loss\n",
    "        mean_loss = epoch_loss / (len(ds_subj_train) / ds_step_size)\n",
    "        if mean_loss < min_loss[0]:\n",
    "            min_loss = (mean_loss, epoch)\n",
    "        model.eval()\n",
    "        common_acc = val(model, dl_test) / len(ds_subj_test)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", common_acc, epoch)\n",
    "        if common_acc > best_rate[0]:\n",
    "            best_rate = (common_acc, epoch)\n",
    "            save_data(best_name, net.state_dict(), epoch, common_acc)\n",
    "        if common_acc < worst_rate[0]:\n",
    "            worst_rate = (common_acc, epoch)\n",
    "        save_data(dump_name, net.state_dict(), epoch, common_acc)\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch:3d}] rate: {common_acc:.4f}; {best_rate = }, {worst_rate = }\"\n",
    "        )\n",
    "    print(\"Finished Training. Min_loss:\", min_loss)\n",
    "    return worst_rate, best_rate, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b972d1c7a24d65a38492b6787b8781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0] rate: 0.6618; best_rate = (tensor(0.6618), 0), worst_rate = (tensor(0.6618), 0)\n",
      "[  1] rate: 0.6765; best_rate = (tensor(0.6765), 1), worst_rate = (tensor(0.6618), 0)\n",
      "[  2] rate: 0.7059; best_rate = (tensor(0.7059), 2), worst_rate = (tensor(0.6618), 0)\n",
      "[  3] rate: 0.7059; best_rate = (tensor(0.7059), 2), worst_rate = (tensor(0.6618), 0)\n",
      "[  4] rate: 0.7647; best_rate = (tensor(0.7647), 4), worst_rate = (tensor(0.6618), 0)\n",
      "[  5] rate: 0.7206; best_rate = (tensor(0.7647), 4), worst_rate = (tensor(0.6618), 0)\n",
      "[  6] rate: 0.7647; best_rate = (tensor(0.7647), 4), worst_rate = (tensor(0.6618), 0)\n",
      "[  7] rate: 0.7794; best_rate = (tensor(0.7794), 7), worst_rate = (tensor(0.6618), 0)\n",
      "[  8] rate: 0.8235; best_rate = (tensor(0.8235), 8), worst_rate = (tensor(0.6618), 0)\n",
      "[  9] rate: 0.8088; best_rate = (tensor(0.8235), 8), worst_rate = (tensor(0.6618), 0)\n",
      "[ 10] rate: 0.7941; best_rate = (tensor(0.8235), 8), worst_rate = (tensor(0.6618), 0)\n",
      "[ 11] rate: 0.8971; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 12] rate: 0.8529; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 13] rate: 0.8676; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 14] rate: 0.8235; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 15] rate: 0.8529; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 16] rate: 0.8382; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 17] rate: 0.8971; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 18] rate: 0.8971; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 19] rate: 0.8971; best_rate = (tensor(0.8971), 11), worst_rate = (tensor(0.6618), 0)\n",
      "[ 20] rate: 0.9265; best_rate = (tensor(0.9265), 20), worst_rate = (tensor(0.6618), 0)\n",
      "[ 21] rate: 0.9265; best_rate = (tensor(0.9265), 20), worst_rate = (tensor(0.6618), 0)\n",
      "[ 22] rate: 0.9412; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 23] rate: 0.8971; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 24] rate: 0.8971; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 25] rate: 0.8971; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 26] rate: 0.8824; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 27] rate: 0.9118; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 28] rate: 0.9412; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "[ 29] rate: 0.9412; best_rate = (tensor(0.9412), 22), worst_rate = (tensor(0.6618), 0)\n",
      "Finished Training. Min_loss: (inf, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor(0.6618), 0), (tensor(0.9412), 22), (inf, 0))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(net, epoch_count=epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9412)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = val(net, dl_test)\n",
    "v / len(ds_subj_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
